{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\envs\\dlwin36\\lib\\site-packages\\sklearn\\cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from driver.MongoDriver import MongoDriver\n",
    "import util.Constants as const\n",
    "from service.DataPreparationHandler import get_data\n",
    "from util.PandasUtils import PandasUtils\n",
    "import DisplayHelper\n",
    "from DisplayHelper import *\n",
    "from pprint import PrettyPrinter\n",
    "from util.DataPreparationUtils import *\n",
    "from stubutils.StubUtils import open_file\n",
    "import pandas\n",
    "from datetime import datetime\n",
    "from scipy import stats\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.ensemble import RandomForestClassifier as rfc\n",
    "from sklearn.preprocessing import OneHotEncoder as ohe\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "mpl.style.use( 'ggplot' )\n",
    "sns.set_style( 'white' )\n",
    "pylab.rcParams[ 'figure.figsize' ] = 8 , 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_instance = MongoDriver.get_instance().get_db_instance(const.DB_INSTANCE)\n",
    "data = get_data(db_instance, 'normalized_data')\n",
    "dataframe = PandasUtils.get_dataframe(data, const.JSON_STRUCTURE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Create delta columns for fields \"from\" and \"to\"\n",
    "dataframe['search_volume'] = (dataframe['search_volume_to'] + dataframe['search_volume_from'])/2\n",
    "dataframe['search_milleage'] = (dataframe['search_milleage_to'] + dataframe['search_milleage_from'])/2\n",
    "dataframe['search_price'] = (dataframe['search_price_to'] + dataframe['search_price_from'])/2\n",
    "dataframe['search_year'] = (dataframe['search_year_to'] + dataframe['search_year_from'])/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in const.NUMERIC_COLUMNS_REQUIRE_ANALYSIS:\n",
    "        quartiles = get_percentiles_for_numeric_column(dataframe,column)\n",
    "        dataframe.loc[ dataframe[column] <= quartiles['values'][1], column] = 0\n",
    "        dataframe.loc[(dataframe[column] > quartiles['values'][1]) & (dataframe[column] <= quartiles['values'][2]), column] = 1\n",
    "        dataframe.loc[(dataframe[column] > quartiles['values'][2]) & (dataframe[column] <= quartiles['values'][3]), column]   = 2\n",
    "        dataframe.loc[ dataframe[column] > quartiles['values'][3], column] = 3\n",
    "        dataframe[column] = dataframe[column].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataframe['search_model'] = dataframe.apply(lambda row: get_id_by_make_name(row['search_model']),axis=1).astype(int)\n",
    "dataframe['search_rigion'] = dataframe.apply(lambda row: get_id_by_region_name(row['search_rigion']),axis=1).astype(int)\n",
    "dataframe['search_city'] = dataframe.apply(lambda row: get_id_by_city_name(row['search_city']),axis=1).astype(int)\n",
    "dataframe['search_country'] = dataframe.apply(lambda row: get_id_by_name_simple(const.COUNTRY_ID_MAPPING_JSON_ROUTE, row['search_country']),axis=1).astype(int)\n",
    "dataframe['search_marka'] = dataframe.apply(lambda row: get_id_by_name_simple(const.MODELS_ID_MAPPING_JSON_ROUTE, row['search_marka']),axis=1).astype(int)\n",
    "dataframe['search_wheel'] = dataframe.apply(lambda row: get_id_by_name_simple(const.WHEELS_ID_MAPPING_JSON_ROUTE, row['search_wheel']),axis=1).astype(int)\n",
    "dataframe['search_body'] = dataframe.apply(lambda row: get_id_by_name_simple(const.BODIES_ID_MAPPING_JSON_ROUTE, row['search_body']),axis=1).astype(int)\n",
    "dataframe['search_transmission'] = dataframe.apply(lambda row: get_id_by_name_simple(const.TRANSMISSIONS_ID_MAPPING_JSON_ROUTE, row['search_transmission']),axis=1).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for column_name in const.DATE_COLUMNS:\n",
    "        dataframe[column_name] = dataframe.apply(lambda row: get_season_by_utcdate(row[column_name]),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Drop unhandled columns\n",
    "for column in const.COLUMNS_TO_DROP:\n",
    "    dataframe = dataframe.drop(column, axis=1)\n",
    "    \n",
    "# dataframe = dataframe.drop(\"search_city\", axis=1)\n",
    "#dataframe = dataframe.drop(\"search_rigion\", axis=1)\n",
    "dataframe = dataframe.drop(\"search_model\", axis=1)\n",
    "#dataframe = dataframe.drop(\"search_year\", axis=1)\n",
    "#dataframe = dataframe.drop(\"search_volume\", axis=1)\n",
    "#dataframe = dataframe.drop(\"search_milleage\", axis=1)\n",
    "#dataframe = dataframe.drop(\"search_price\", axis=1)\n",
    "#dataframe = dataframe.drop(\"search_year\", axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataframe[\"a\"] = dataframe.apply(lambda row: (row['search_volume'] + 1)/(row['search_milleage']+1),axis=1).astype(int)\n",
    "dataframe[\"b\"] = dataframe.apply(lambda row: (row['search_volume'] + 1)/(row['search_price']+1),axis=1).astype(int)\n",
    "dataframe[\"c\"] = dataframe.apply(lambda row: (row['search_volume'] + 1)/(row['search_year']+1),axis=1).astype(int)\n",
    "dataframe[\"d\"] = dataframe.apply(lambda row: (row['search_milleage'] + 1)/(row['search_volume']+1),axis=1).astype(int)\n",
    "dataframe[\"f\"] = dataframe.apply(lambda row: (row['search_milleage'] + 1)/(row['search_price']+1),axis=1).astype(int)\n",
    "dataframe[\"e\"] = dataframe.apply(lambda row: (row['search_milleage'] + 1)/(row['search_year']+1),axis=1).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_data = dataframe.drop(\"search_marka\", axis=1)          # data: Features\n",
    "Y_data = dataframe[\"search_marka\"]                       # data: Labels\n",
    "for f in X_data.columns: \n",
    "    if X_data[f].dtype=='object': \n",
    "        lbl = preprocessing.LabelEncoder() \n",
    "        lbl.fit(list(X_data[f].values)) \n",
    "        X_data[f] = lbl.transform(list(X_data[f].values))\n",
    "#split data into random \"train\" and \"test\" set \n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X_data, Y_data, test_size=0.2, random_state=0)\n",
    "X_train = X_train.drop('user_id', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_ref = X_test.drop('user_id', axis=1)\n",
    "xg_train = xgb.DMatrix(X_train, label=Y_train)\n",
    "xg_test = xgb.DMatrix(X_test_ref, label=Y_test)\n",
    "# setup parameters for xgboost\n",
    "param = {}\n",
    "# use softmax multi-class classification\n",
    "param['objective'] = 'multi:softmax'\n",
    "# scale weight of positive examples\n",
    "param['eta'] = 0.1\n",
    "param['max_depth'] = 4\n",
    "param['silent'] = 1\n",
    "param['nthread'] = 4\n",
    "param['subsample'] = 0.6\n",
    "# param['min_child_weight'] = 100\n",
    "param['num_class'] = 150\n",
    "\n",
    "res = xgb.cv(param, xg_train, nfold = 5, num_boost_round = 100, stratified = True, verbose_eval = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_ref = X_test.drop('user_id', axis=1)\n",
    "xg_train = xgb.DMatrix(X_train, label=Y_train)\n",
    "xg_test = xgb.DMatrix(X_test_ref, label=Y_test)\n",
    "# setup parameters for xgboost\n",
    "param = {}\n",
    "# use softmax multi-class classification\n",
    "param['objective'] = 'multi:softmax'\n",
    "# scale weight of positive examples\n",
    "param['eta'] = 0.1\n",
    "param['max_depth'] = 4\n",
    "param['silent'] = 1\n",
    "param['nthread'] = 4\n",
    "param['num_class'] = 150\n",
    "watchlist = [(xg_train, 'train'), (xg_test, 'test')]\n",
    "num_round = 3000\n",
    "bst = xgb.train(param, xg_train, num_round, watchlist)\n",
    "# get prediction\n",
    "pred = bst.predict(xg_test, ntree_limit=bst.best_iteration)\n",
    "# error_rate = np.sum(pred != Y_test) / Y_test.shape[0]\n",
    "print('Accuracy score = {}'.format(accuracy_score(Y_test, pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training finished in 331 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.016666666666666666"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "param_dist = {'n_estimators':7, 'nthread':22, 'max_depth':7}\n",
    "xg = xgb.XGBClassifier()\n",
    "d1 = datetime.now()\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X_data, Y_data, test_size=0.2, random_state=0)\n",
    "X_train = X_train.drop('user_id', axis=1)\n",
    "for i in range(1,60):\n",
    "    X_inner_train, X_inner_test, Y_inner_train, Y_inner_test = train_test_split(X_data, Y_data, test_size=0.2, random_state=0)\n",
    "    X_inner_train = X_inner_train.drop('user_id', axis=1)\n",
    "    xg.fit(X_inner_train, Y_inner_train, verbose=True)\n",
    "d2 = datetime.now()\n",
    "delta = d2 - d1\n",
    "print \"training finished in \" + str(delta.seconds) + \" seconds\"\n",
    "X_test_ref = X_test.drop('user_id', axis=1)\n",
    "Y_pred  = xg.predict(X_test_ref)\n",
    "accuracy_score(Y_test, Y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for f in X_train.columns: \n",
    "    if X_train[f].dtype=='object': \n",
    "        lbl = preprocessing.LabelEncoder() \n",
    "        lbl.fit(list(X_train[f].values)) \n",
    "        X_train[f] = lbl.transform(list(X_train[f].values))\n",
    "for f in X_test.columns: \n",
    "    if X_test[f].dtype=='object': \n",
    "        lbl = preprocessing.LabelEncoder() \n",
    "        lbl.fit(list(X_test[f].values)) \n",
    "        X_test[f] = lbl.transform(list(X_test[f].values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training finished in 3 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.016666666666666666"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "# Support Vector Machines\n",
    "d1 = datetime.now()\n",
    "param_dist = {'n_estimators':355, 'nthread':22, 'max_depth':7}\n",
    "xg = xgb.XGBClassifier()\n",
    "xg.fit(X_train, Y_train, verbose=True)\n",
    "d2 = datetime.now()\n",
    "delta = d2 - d1\n",
    "print \"training finished in \" + str(delta.seconds) + \" seconds\"\n",
    "X_test_ref = X_test.drop('user_id', axis=1)\n",
    "Y_pred  = xg.predict(X_test_ref)\n",
    "\n",
    "submission = pd.DataFrame({\n",
    "         \"user_id\": X_test[\"user_id\"],\n",
    "         \"search_marka\": Y_pred\n",
    "     })\n",
    "submission.to_csv('resource/results_XGB.csv', index=False)\n",
    "accuracy_score(Y_test, Y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "dataframe = pandas.get_dummies(dataframe, columns=[\"search_rigion\", \"search_country\", \"search_city\", \"search_body\", \"search_transmission\", \"search_wheel\"], \n",
    "                   prefix=[\"rigion\", \"country\", \"city\",\"body\",\"transmission\",\"wheel\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataframe['search_marka'] = dataframe.apply(lambda row: get_id_by_name_simple(const.MODELS_ID_MAPPING_JSON_ROUTE, row['search_marka']),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "import pandas\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.utils import np_utils\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from keras import optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# fix random seed for reproducibility\n",
    "seed = 50\n",
    "numpy.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load dataset\n",
    "dataframe = pandas.read_csv(\"test.csv\", header=None)\n",
    "dataset = dataframe.values\n",
    "X = dataset[:,0:4].astype(float)\n",
    "Y = dataset[:,4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_data = dataframe.drop(\"search_marka\", axis=1)          # data: Features\n",
    "Y_data = dataframe[\"search_marka\"]                       # data: Labels\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X_data, Y_data, test_size=0.2, random_state=0)\n",
    "\n",
    "X = X_train.drop(['user_id'], axis=1).values\n",
    "X_test = X_test.drop(['user_id'], axis=1).values\n",
    "Y = Y_train.values\n",
    "Y_test = Y_test.values\n",
    "\n",
    "# encode class values as integers\n",
    "uniques_train, ids_train = np.unique(Y_train, return_inverse=True)\n",
    "uniques_test, ids_test = np.unique(Y_test, return_inverse=True)\n",
    "# convert integers to dummy variables (i.e. one hot encoded)\n",
    "dummy_train = np_utils.to_categorical(ids_train, len(Y_data.unique()))\n",
    "# convert integers to dummy variables (i.e. one hot encoded)\n",
    "dummy_test = np_utils.to_categorical(ids_test, len(Y_data.unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "set()"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(uniques_train) - set(uniques_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uniques_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import Callback\n",
    "class EarlyStoppingByLossVal(Callback):\n",
    "    def __init__(self, monitor='val_loss', value=0.00001, verbose=0):\n",
    "        super(Callback, self).__init__()\n",
    "        self.monitor = monitor\n",
    "        self.value = value\n",
    "        self.verbose = verbose\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        current = logs.get(self.monitor)\n",
    "        if current is None:\n",
    "            warnings.warn(\"Early stopping requires %s available!\" % self.monitor, RuntimeWarning)\n",
    "\n",
    "        if current > self.value:\n",
    "            if self.verbose > 0:\n",
    "                print(\"Epoch %05d: early stopping THR\" % epoch)\n",
    "            self.model.stop_training = True\n",
    "callbacks = [\n",
    "    EarlyStoppingByLossVal(monitor='val_acc', value=0.92, verbose=1),\n",
    "    # EarlyStopping(monitor='val_loss', patience=2, verbose=0),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define baseline model\n",
    "def baseline_model():\n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(Dense(X.shape[1]-1, input_dim=X.shape[1], activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(X.shape[1]-2, input_dim=X.shape[1], activation='relu'))\n",
    "    model.add(Dense(X.shape[1]-1, input_dim=X.shape[1], activation='relu'))\n",
    "    model.add(Dense(X.shape[1]-1, input_dim=X.shape[1], activation='relu'))\n",
    "#     model.add(Dense(8, input_dim=4, activation='relu'))\n",
    "    model.add(Dense(len(dataframe['search_marka'].unique()), activation='softmax'))\n",
    "#     model.add(Dense(3, activation='softmax'))\n",
    "    # Compile model\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Opel', 'Ford', 'Skoda', ..., 'Yugo', 'Cadillac', 'Autobianchi'], dtype=object)"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from numpy import argmax\n",
    "dummy = numpy.take(Y_data.unique(), numpy.argmax(dummy_y_test,1))\n",
    "dummy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Saipa', 'Austin', 'SsangYong', ..., 'Saturn', 'Jeep', 'Toyota'], dtype=object)"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_code = np_utils.to_categorical(ids, len(uniques))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = uniques_test[dummy_train.argmax(1)]\n",
    "accuracy_score(Y_train, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 92,   6, 100, ...,  94,  55, 108], dtype=int64)"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "estimator = KerasClassifier(build_fn=baseline_model, epochs=20, batch_size=200, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "kfold = KFold(n_splits=10, shuffle=True, random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7999 samples, validate on 2000 samples\n",
      "Epoch 1/3000\n",
      "7999/7999 [==============================] - 279s - loss: 4.8872 - acc: 0.0080 - val_loss: 4.8828 - val_acc: 0.0065\n",
      "Epoch 2/3000\n",
      "7999/7999 [==============================] - 167s - loss: 4.6773 - acc: 0.0180 - val_loss: 4.2011 - val_acc: 0.0335\n",
      "Epoch 3/3000\n",
      "7999/7999 [==============================] - 161s - loss: 3.8953 - acc: 0.0568 - val_loss: 3.5514 - val_acc: 0.0810\n",
      "Epoch 4/3000\n",
      "7999/7999 [==============================] - 163s - loss: 3.4058 - acc: 0.1046 - val_loss: 3.0176 - val_acc: 0.1625\n",
      "Epoch 5/3000\n",
      "7999/7999 [==============================] - 164s - loss: 2.8697 - acc: 0.1979 - val_loss: 2.6759 - val_acc: 0.2460\n",
      "Epoch 6/3000\n",
      "7999/7999 [==============================] - 160s - loss: 2.3864 - acc: 0.3022 - val_loss: 2.1149 - val_acc: 0.3890\n",
      "Epoch 7/3000\n",
      "7999/7999 [==============================] - 164s - loss: 1.9865 - acc: 0.3905 - val_loss: 1.8500 - val_acc: 0.4390\n",
      "Epoch 8/3000\n",
      "7999/7999 [==============================] - 166s - loss: 1.6793 - acc: 0.4682 - val_loss: 1.5798 - val_acc: 0.5310\n",
      "Epoch 9/3000\n",
      "7999/7999 [==============================] - 168s - loss: 1.4700 - acc: 0.5189 - val_loss: 1.4717 - val_acc: 0.5580\n",
      "Epoch 10/3000\n",
      "7999/7999 [==============================] - 170s - loss: 1.2767 - acc: 0.5776 - val_loss: 1.3650 - val_acc: 0.5815\n",
      "Epoch 11/3000\n",
      "7999/7999 [==============================] - 170s - loss: 1.1210 - acc: 0.6216 - val_loss: 1.3390 - val_acc: 0.6170\n",
      "Epoch 12/3000\n",
      "7999/7999 [==============================] - 170s - loss: 0.9980 - acc: 0.6603 - val_loss: 1.2842 - val_acc: 0.6235\n",
      "Epoch 13/3000\n",
      "7999/7999 [==============================] - 172s - loss: 0.8704 - acc: 0.7013 - val_loss: 1.0936 - val_acc: 0.7025\n",
      "Epoch 14/3000\n",
      "7999/7999 [==============================] - 169s - loss: 0.7912 - acc: 0.7292 - val_loss: 1.1039 - val_acc: 0.6980\n",
      "Epoch 15/3000\n",
      "7999/7999 [==============================] - 168s - loss: 0.6545 - acc: 0.7748 - val_loss: 1.0295 - val_acc: 0.7335\n",
      "Epoch 16/3000\n",
      "7999/7999 [==============================] - 173s - loss: 0.5806 - acc: 0.7976 - val_loss: 0.9534 - val_acc: 0.7695\n",
      "Epoch 17/3000\n",
      "7999/7999 [==============================] - 167s - loss: 0.4951 - acc: 0.8340 - val_loss: 1.0020 - val_acc: 0.7585\n",
      "Epoch 18/3000\n",
      "7999/7999 [==============================] - 156s - loss: 0.4392 - acc: 0.8491 - val_loss: 1.0339 - val_acc: 0.7445\n",
      "Epoch 19/3000\n",
      "7999/7999 [==============================] - 154s - loss: 0.3644 - acc: 0.8769 - val_loss: 0.9721 - val_acc: 0.7830\n",
      "Epoch 20/3000\n",
      "7999/7999 [==============================] - 155s - loss: 0.3363 - acc: 0.8871 - val_loss: 0.9858 - val_acc: 0.7780\n",
      "Epoch 21/3000\n",
      "7999/7999 [==============================] - 154s - loss: 0.3184 - acc: 0.8954 - val_loss: 0.9991 - val_acc: 0.7770\n",
      "Epoch 22/3000\n",
      "7999/7999 [==============================] - 154s - loss: 0.2701 - acc: 0.9106 - val_loss: 1.1374 - val_acc: 0.7600\n",
      "Epoch 23/3000\n",
      "7999/7999 [==============================] - 154s - loss: 0.2512 - acc: 0.9200 - val_loss: 1.0265 - val_acc: 0.7820\n",
      "Epoch 24/3000\n",
      "7999/7999 [==============================] - 154s - loss: 0.2371 - acc: 0.9224 - val_loss: 0.9376 - val_acc: 0.8010\n",
      "Epoch 25/3000\n",
      "7999/7999 [==============================] - 155s - loss: 0.2420 - acc: 0.9237 - val_loss: 1.0178 - val_acc: 0.7845\n",
      "Epoch 26/3000\n",
      "7999/7999 [==============================] - 155s - loss: 0.1984 - acc: 0.9341 - val_loss: 1.0602 - val_acc: 0.7885\n",
      "Epoch 27/3000\n",
      "7999/7999 [==============================] - 154s - loss: 0.1902 - acc: 0.9390 - val_loss: 0.9285 - val_acc: 0.8065\n",
      "Epoch 28/3000\n",
      "7999/7999 [==============================] - 155s - loss: 0.1847 - acc: 0.9422 - val_loss: 1.1068 - val_acc: 0.7795\n",
      "Epoch 29/3000\n",
      "7999/7999 [==============================] - 155s - loss: 0.1719 - acc: 0.9412 - val_loss: 0.8908 - val_acc: 0.8225\n",
      "Epoch 30/3000\n",
      "7999/7999 [==============================] - 155s - loss: 0.1606 - acc: 0.9439 - val_loss: 0.9184 - val_acc: 0.8240\n",
      "Epoch 31/3000\n",
      "7999/7999 [==============================] - 155s - loss: 0.1470 - acc: 0.9506 - val_loss: 1.0284 - val_acc: 0.8160\n",
      "Epoch 32/3000\n",
      "7999/7999 [==============================] - 155s - loss: 0.1635 - acc: 0.9477 - val_loss: 1.1389 - val_acc: 0.7880\n",
      "Epoch 33/3000\n",
      "7999/7999 [==============================] - 154s - loss: 0.1407 - acc: 0.9539 - val_loss: 1.0579 - val_acc: 0.7895\n",
      "Epoch 34/3000\n",
      "7999/7999 [==============================] - 154s - loss: 0.1473 - acc: 0.9517 - val_loss: 0.9392 - val_acc: 0.8285\n",
      "Epoch 35/3000\n",
      "7999/7999 [==============================] - 154s - loss: 0.1374 - acc: 0.9574 - val_loss: 0.9638 - val_acc: 0.8190\n",
      "Epoch 36/3000\n",
      "7999/7999 [==============================] - 154s - loss: 0.1213 - acc: 0.9590 - val_loss: 1.0020 - val_acc: 0.8240\n",
      "Epoch 37/3000\n",
      "7999/7999 [==============================] - 154s - loss: 0.1156 - acc: 0.9614 - val_loss: 1.0090 - val_acc: 0.8260\n",
      "Epoch 38/3000\n",
      "7999/7999 [==============================] - 154s - loss: 0.1332 - acc: 0.9584 - val_loss: 1.0807 - val_acc: 0.8030\n",
      "Epoch 39/3000\n",
      "7999/7999 [==============================] - 154s - loss: 0.1053 - acc: 0.9655 - val_loss: 1.0200 - val_acc: 0.8240\n",
      "Epoch 40/3000\n",
      "7999/7999 [==============================] - 154s - loss: 0.1129 - acc: 0.9654 - val_loss: 1.0730 - val_acc: 0.8265\n",
      "Epoch 41/3000\n",
      "7999/7999 [==============================] - 154s - loss: 0.1167 - acc: 0.9644 - val_loss: 1.0139 - val_acc: 0.8290\n",
      "Epoch 42/3000\n",
      "7999/7999 [==============================] - 155s - loss: 0.1044 - acc: 0.9672 - val_loss: 1.1868 - val_acc: 0.8050\n",
      "Epoch 43/3000\n",
      "7999/7999 [==============================] - 153s - loss: 0.1317 - acc: 0.9581 - val_loss: 0.9646 - val_acc: 0.8200\n",
      "Epoch 44/3000\n",
      "7999/7999 [==============================] - 154s - loss: 0.0798 - acc: 0.9765 - val_loss: 0.9668 - val_acc: 0.8360\n",
      "Epoch 45/3000\n",
      "7999/7999 [==============================] - 155s - loss: 0.1306 - acc: 0.9620 - val_loss: 1.1745 - val_acc: 0.7995\n",
      "Epoch 46/3000\n",
      "7999/7999 [==============================] - 154s - loss: 0.1275 - acc: 0.9615 - val_loss: 1.0428 - val_acc: 0.8180\n",
      "Epoch 47/3000\n",
      "7999/7999 [==============================] - 154s - loss: 0.1055 - acc: 0.9660 - val_loss: 0.9266 - val_acc: 0.8275\n",
      "Epoch 48/3000\n",
      "7999/7999 [==============================] - 156s - loss: 0.0754 - acc: 0.9762 - val_loss: 1.0698 - val_acc: 0.8245\n",
      "Epoch 49/3000\n",
      "7999/7999 [==============================] - 155s - loss: 0.0776 - acc: 0.9765 - val_loss: 0.9034 - val_acc: 0.8495\n",
      "Epoch 50/3000\n",
      "7999/7999 [==============================] - 154s - loss: 0.0890 - acc: 0.9717 - val_loss: 0.9892 - val_acc: 0.8265\n",
      "Epoch 51/3000\n",
      "7999/7999 [==============================] - 155s - loss: 0.1017 - acc: 0.9671 - val_loss: 1.0258 - val_acc: 0.8315\n",
      "Epoch 52/3000\n",
      "7999/7999 [==============================] - 154s - loss: 0.0770 - acc: 0.9742 - val_loss: 0.9102 - val_acc: 0.8485\n",
      "Epoch 53/3000\n",
      "7999/7999 [==============================] - 154s - loss: 0.0885 - acc: 0.9739 - val_loss: 0.9997 - val_acc: 0.8400\n",
      "Epoch 54/3000\n",
      "7999/7999 [==============================] - 154s - loss: 0.0794 - acc: 0.9739 - val_loss: 0.9895 - val_acc: 0.8415\n",
      "Epoch 55/3000\n",
      "7999/7999 [==============================] - 154s - loss: 0.0840 - acc: 0.9730 - val_loss: 1.1016 - val_acc: 0.8280\n",
      "Epoch 56/3000\n",
      "7999/7999 [==============================] - 153s - loss: 0.1171 - acc: 0.9652 - val_loss: 1.0748 - val_acc: 0.8210\n",
      "Epoch 57/3000\n",
      "7999/7999 [==============================] - 153s - loss: 0.0778 - acc: 0.9756 - val_loss: 0.9953 - val_acc: 0.8445\n",
      "Epoch 58/3000\n",
      "7999/7999 [==============================] - 154s - loss: 0.0711 - acc: 0.9797 - val_loss: 0.9364 - val_acc: 0.8470\n",
      "Epoch 59/3000\n",
      "7999/7999 [==============================] - 153s - loss: 0.1047 - acc: 0.9691 - val_loss: 1.1136 - val_acc: 0.8075\n",
      "Epoch 60/3000\n",
      "7999/7999 [==============================] - 153s - loss: 0.0816 - acc: 0.9746 - val_loss: 1.0148 - val_acc: 0.8210\n",
      "Epoch 61/3000\n",
      "7999/7999 [==============================] - 154s - loss: 0.0679 - acc: 0.9794 - val_loss: 1.0363 - val_acc: 0.8315\n",
      "Epoch 62/3000\n",
      "7999/7999 [==============================] - 153s - loss: 0.0828 - acc: 0.9752 - val_loss: 0.9928 - val_acc: 0.8360\n",
      "Epoch 63/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7999/7999 [==============================] - 153s - loss: 0.0589 - acc: 0.9799 - val_loss: 1.0458 - val_acc: 0.8400\n",
      "Epoch 64/3000\n",
      "7999/7999 [==============================] - 153s - loss: 0.0864 - acc: 0.9741 - val_loss: 0.9498 - val_acc: 0.8480\n",
      "Epoch 65/3000\n",
      "7999/7999 [==============================] - 152s - loss: 0.0601 - acc: 0.9809 - val_loss: 1.0026 - val_acc: 0.8420\n",
      "Epoch 66/3000\n",
      "7999/7999 [==============================] - 154s - loss: 0.0675 - acc: 0.9787 - val_loss: 0.9950 - val_acc: 0.8415\n",
      "Epoch 67/3000\n",
      "7999/7999 [==============================] - 153s - loss: 0.0492 - acc: 0.9840 - val_loss: 0.9604 - val_acc: 0.8475\n",
      "Epoch 68/3000\n",
      "7999/7999 [==============================] - 153s - loss: 0.0613 - acc: 0.9807 - val_loss: 0.9687 - val_acc: 0.8460\n",
      "Epoch 69/3000\n",
      "7999/7999 [==============================] - 154s - loss: 0.0735 - acc: 0.9771 - val_loss: 0.9645 - val_acc: 0.8450\n",
      "Epoch 70/3000\n",
      "7999/7999 [==============================] - 153s - loss: 0.0610 - acc: 0.9812 - val_loss: 1.0766 - val_acc: 0.8405\n",
      "Epoch 71/3000\n",
      "7999/7999 [==============================] - 152s - loss: 0.0814 - acc: 0.9780 - val_loss: 0.9651 - val_acc: 0.8495\n",
      "Epoch 72/3000\n",
      "7999/7999 [==============================] - 153s - loss: 0.0776 - acc: 0.9776 - val_loss: 0.9815 - val_acc: 0.8510\n",
      "Epoch 73/3000\n",
      "7999/7999 [==============================] - 153s - loss: 0.0584 - acc: 0.9829 - val_loss: 1.0118 - val_acc: 0.8450\n",
      "Epoch 74/3000\n",
      "7999/7999 [==============================] - 153s - loss: 0.0585 - acc: 0.9815 - val_loss: 0.9446 - val_acc: 0.8565\n",
      "Epoch 75/3000\n",
      "7999/7999 [==============================] - 153s - loss: 0.0697 - acc: 0.9797 - val_loss: 0.9620 - val_acc: 0.8470\n",
      "Epoch 76/3000\n",
      "7999/7999 [==============================] - 153s - loss: 0.0651 - acc: 0.9815 - val_loss: 0.9816 - val_acc: 0.8515\n",
      "Epoch 77/3000\n",
      "7999/7999 [==============================] - 154s - loss: 0.0771 - acc: 0.9761 - val_loss: 1.0823 - val_acc: 0.8530\n",
      "Epoch 78/3000\n",
      "7999/7999 [==============================] - 153s - loss: 0.0636 - acc: 0.9820 - val_loss: 1.1112 - val_acc: 0.8305\n",
      "Epoch 79/3000\n",
      "7999/7999 [==============================] - 152s - loss: 0.0476 - acc: 0.9852 - val_loss: 0.9829 - val_acc: 0.8600\n",
      "Epoch 80/3000\n",
      "7999/7999 [==============================] - 153s - loss: 0.0499 - acc: 0.9862 - val_loss: 1.0397 - val_acc: 0.8465\n",
      "Epoch 81/3000\n",
      "7999/7999 [==============================] - 153s - loss: 0.0522 - acc: 0.9846 - val_loss: 0.9878 - val_acc: 0.8510\n",
      "Epoch 82/3000\n",
      "7999/7999 [==============================] - 153s - loss: 0.0478 - acc: 0.9837 - val_loss: 1.0496 - val_acc: 0.8480\n",
      "Epoch 83/3000\n",
      "7999/7999 [==============================] - 153s - loss: 0.0538 - acc: 0.9834 - val_loss: 1.0135 - val_acc: 0.8515\n",
      "Epoch 84/3000\n",
      "7999/7999 [==============================] - 154s - loss: 0.0662 - acc: 0.9816 - val_loss: 1.1002 - val_acc: 0.8475\n",
      "Epoch 85/3000\n",
      "7999/7999 [==============================] - 158s - loss: 0.0467 - acc: 0.9856 - val_loss: 1.0555 - val_acc: 0.8450\n",
      "Epoch 86/3000\n",
      "7999/7999 [==============================] - 156s - loss: 0.0561 - acc: 0.9844 - val_loss: 1.0746 - val_acc: 0.8470\n",
      "Epoch 87/3000\n",
      "7999/7999 [==============================] - 156s - loss: 0.0584 - acc: 0.9839 - val_loss: 1.1639 - val_acc: 0.8395\n",
      "Epoch 88/3000\n",
      "7999/7999 [==============================] - 156s - loss: 0.0586 - acc: 0.9830 - val_loss: 0.9653 - val_acc: 0.8550\n",
      "Epoch 89/3000\n",
      "7999/7999 [==============================] - 157s - loss: 0.0867 - acc: 0.9749 - val_loss: 1.1109 - val_acc: 0.8310\n",
      "Epoch 90/3000\n",
      "7999/7999 [==============================] - 155s - loss: 0.0408 - acc: 0.9864 - val_loss: 0.9679 - val_acc: 0.8560\n",
      "Epoch 91/3000\n",
      "7999/7999 [==============================] - 155s - loss: 0.0691 - acc: 0.9804 - val_loss: 1.0350 - val_acc: 0.8450\n",
      "Epoch 92/3000\n",
      "7999/7999 [==============================] - 156s - loss: 0.0516 - acc: 0.9844 - val_loss: 1.0104 - val_acc: 0.8470\n",
      "Epoch 93/3000\n",
      "7999/7999 [==============================] - 155s - loss: 0.0625 - acc: 0.9829 - val_loss: 1.0052 - val_acc: 0.8490\n",
      "Epoch 94/3000\n",
      "7999/7999 [==============================] - 157s - loss: 0.0447 - acc: 0.9861 - val_loss: 0.9377 - val_acc: 0.8650\n",
      "Epoch 95/3000\n",
      "7999/7999 [==============================] - 155s - loss: 0.0516 - acc: 0.9860 - val_loss: 1.1132 - val_acc: 0.8325\n",
      "Epoch 96/3000\n",
      "7999/7999 [==============================] - 155s - loss: 0.0680 - acc: 0.9826 - val_loss: 1.0562 - val_acc: 0.8390\n",
      "Epoch 97/3000\n",
      "7999/7999 [==============================] - 157s - loss: 0.0669 - acc: 0.9815 - val_loss: 0.9987 - val_acc: 0.8530\n",
      "Epoch 98/3000\n",
      "7999/7999 [==============================] - 157s - loss: 0.0493 - acc: 0.9835 - val_loss: 0.9639 - val_acc: 0.8595\n",
      "Epoch 99/3000\n",
      "7999/7999 [==============================] - 156s - loss: 0.0553 - acc: 0.9837 - val_loss: 1.0160 - val_acc: 0.8575\n",
      "Epoch 100/3000\n",
      "7999/7999 [==============================] - 154s - loss: 0.0508 - acc: 0.9836 - val_loss: 0.9250 - val_acc: 0.8635\n",
      "Epoch 101/3000\n",
      "7999/7999 [==============================] - 157s - loss: 0.0470 - acc: 0.9882 - val_loss: 1.0165 - val_acc: 0.8540\n",
      "Epoch 102/3000\n",
      "7999/7999 [==============================] - 156s - loss: 0.0492 - acc: 0.9885 - val_loss: 1.0234 - val_acc: 0.8545\n",
      "Epoch 103/3000\n",
      "7999/7999 [==============================] - 158s - loss: 0.0394 - acc: 0.9877 - val_loss: 1.0535 - val_acc: 0.8620\n",
      "Epoch 104/3000\n",
      "7999/7999 [==============================] - 157s - loss: 0.0332 - acc: 0.9905 - val_loss: 1.0865 - val_acc: 0.8530\n",
      "Epoch 105/3000\n",
      "7999/7999 [==============================] - 157s - loss: 0.0304 - acc: 0.9899 - val_loss: 1.0605 - val_acc: 0.8530\n",
      "Epoch 106/3000\n",
      "7999/7999 [==============================] - 157s - loss: 0.0591 - acc: 0.9841 - val_loss: 0.9694 - val_acc: 0.8510\n",
      "Epoch 107/3000\n",
      "7999/7999 [==============================] - 157s - loss: 0.0263 - acc: 0.9914 - val_loss: 0.9799 - val_acc: 0.8650\n",
      "Epoch 108/3000\n",
      "7999/7999 [==============================] - 158s - loss: 0.0402 - acc: 0.9895 - val_loss: 1.0430 - val_acc: 0.8490\n",
      "Epoch 109/3000\n",
      "7999/7999 [==============================] - 157s - loss: 0.0524 - acc: 0.9852 - val_loss: 1.0995 - val_acc: 0.8465\n",
      "Epoch 110/3000\n",
      "7999/7999 [==============================] - 156s - loss: 0.0605 - acc: 0.9829 - val_loss: 1.0285 - val_acc: 0.8475\n",
      "Epoch 111/3000\n",
      "7999/7999 [==============================] - 153s - loss: 0.0525 - acc: 0.9884 - val_loss: 1.0869 - val_acc: 0.8460\n",
      "Epoch 112/3000\n",
      "7999/7999 [==============================] - 154s - loss: 0.0683 - acc: 0.9840 - val_loss: 0.9229 - val_acc: 0.8670\n",
      "Epoch 113/3000\n",
      "7999/7999 [==============================] - 154s - loss: 0.0388 - acc: 0.9862 - val_loss: 0.9949 - val_acc: 0.8550\n",
      "Epoch 114/3000\n",
      "7999/7999 [==============================] - 154s - loss: 0.0328 - acc: 0.9897 - val_loss: 1.1020 - val_acc: 0.8540\n",
      "Epoch 115/3000\n",
      "7999/7999 [==============================] - 153s - loss: 0.0451 - acc: 0.9879 - val_loss: 1.0945 - val_acc: 0.8435\n",
      "Epoch 116/3000\n",
      "7999/7999 [==============================] - 153s - loss: 0.0657 - acc: 0.9817 - val_loss: 0.9950 - val_acc: 0.8590\n",
      "Epoch 117/3000\n",
      "7999/7999 [==============================] - 154s - loss: 0.0618 - acc: 0.9851 - val_loss: 0.9653 - val_acc: 0.8585\n",
      "Epoch 118/3000\n",
      "7999/7999 [==============================] - 154s - loss: 0.0453 - acc: 0.9872 - val_loss: 1.0438 - val_acc: 0.8455\n",
      "Epoch 119/3000\n",
      "7999/7999 [==============================] - 154s - loss: 0.0248 - acc: 0.9916 - val_loss: 0.9717 - val_acc: 0.8590\n",
      "Epoch 120/3000\n",
      "7999/7999 [==============================] - 155s - loss: 0.0370 - acc: 0.9885 - val_loss: 0.9983 - val_acc: 0.8605\n",
      "Epoch 121/3000\n",
      "4000/7999 [==============>...............] - ETA: 77s - loss: 0.0488 - acc: 0.9878"
     ]
    },
    {
     "ename": "ResourceExhaustedError",
     "evalue": "OOM when allocating tensor with shape[3639,3638]\n\t [[Node: mul_523 = Mul[T=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](sub_315, gradients_12/dense_62/MatMul_grad/MatMul_1)]]\n\nCaused by op 'mul_523', defined at:\n  File \"D:\\Anaconda3\\envs\\dlwin36\\lib\\runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"D:\\Anaconda3\\envs\\dlwin36\\lib\\runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"D:\\Anaconda3\\envs\\dlwin36\\lib\\site-packages\\ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"D:\\Anaconda3\\envs\\dlwin36\\lib\\site-packages\\traitlets\\config\\application.py\", line 658, in launch_instance\n    app.start()\n  File \"D:\\Anaconda3\\envs\\dlwin36\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 477, in start\n    ioloop.IOLoop.instance().start()\n  File \"D:\\Anaconda3\\envs\\dlwin36\\lib\\site-packages\\zmq\\eventloop\\ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"D:\\Anaconda3\\envs\\dlwin36\\lib\\site-packages\\tornado\\ioloop.py\", line 888, in start\n    handler_func(fd_obj, events)\n  File \"D:\\Anaconda3\\envs\\dlwin36\\lib\\site-packages\\tornado\\stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"D:\\Anaconda3\\envs\\dlwin36\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"D:\\Anaconda3\\envs\\dlwin36\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"D:\\Anaconda3\\envs\\dlwin36\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"D:\\Anaconda3\\envs\\dlwin36\\lib\\site-packages\\tornado\\stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"D:\\Anaconda3\\envs\\dlwin36\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"D:\\Anaconda3\\envs\\dlwin36\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 235, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"D:\\Anaconda3\\envs\\dlwin36\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"D:\\Anaconda3\\envs\\dlwin36\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"D:\\Anaconda3\\envs\\dlwin36\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 533, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"D:\\Anaconda3\\envs\\dlwin36\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2698, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"D:\\Anaconda3\\envs\\dlwin36\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2808, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"D:\\Anaconda3\\envs\\dlwin36\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2862, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-60-3682969d6ad7>\", line 2, in <module>\n    model.fit(X, dummy_train, batch_size=50, epochs=3000, validation_data=(X_test, dummy_test), callbacks=callbacks)\n  File \"D:\\Anaconda3\\envs\\dlwin36\\lib\\site-packages\\keras\\models.py\", line 870, in fit\n    initial_epoch=initial_epoch)\n  File \"D:\\Anaconda3\\envs\\dlwin36\\lib\\site-packages\\keras\\engine\\training.py\", line 1490, in fit\n    self._make_train_function()\n  File \"D:\\Anaconda3\\envs\\dlwin36\\lib\\site-packages\\keras\\engine\\training.py\", line 1014, in _make_train_function\n    self.total_loss)\n  File \"D:\\Anaconda3\\envs\\dlwin36\\lib\\site-packages\\keras\\optimizers.py\", line 422, in get_updates\n    m_t = (self.beta_1 * m) + (1. - self.beta_1) * g\n  File \"D:\\Anaconda3\\envs\\dlwin36\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py\", line 838, in binary_op_wrapper\n    return func(x, y, name=name)\n  File \"D:\\Anaconda3\\envs\\dlwin36\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py\", line 1061, in _mul_dispatch\n    return gen_math_ops._mul(x, y, name=name)\n  File \"D:\\Anaconda3\\envs\\dlwin36\\lib\\site-packages\\tensorflow\\python\\ops\\gen_math_ops.py\", line 1377, in _mul\n    result = _op_def_lib.apply_op(\"Mul\", x=x, y=y, name=name)\n  File \"D:\\Anaconda3\\envs\\dlwin36\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 767, in apply_op\n    op_def=op_def)\n  File \"D:\\Anaconda3\\envs\\dlwin36\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 2506, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"D:\\Anaconda3\\envs\\dlwin36\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1269, in __init__\n    self._traceback = _extract_stack()\n\nResourceExhaustedError (see above for traceback): OOM when allocating tensor with shape[3639,3638]\n\t [[Node: mul_523 = Mul[T=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](sub_315, gradients_12/dense_62/MatMul_grad/MatMul_1)]]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[1;32mD:\\Anaconda3\\envs\\dlwin36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1138\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1139\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1140\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\envs\\dlwin36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1120\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1121\u001b[1;33m                                  status, run_metadata)\n\u001b[0m\u001b[0;32m   1122\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\envs\\dlwin36\\lib\\contextlib.py\u001b[0m in \u001b[0;36m__exit__\u001b[1;34m(self, type, value, traceback)\u001b[0m\n\u001b[0;32m     87\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 88\u001b[1;33m                 \u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     89\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\envs\\dlwin36\\lib\\site-packages\\tensorflow\\python\\framework\\errors_impl.py\u001b[0m in \u001b[0;36mraise_exception_on_not_ok_status\u001b[1;34m()\u001b[0m\n\u001b[0;32m    465\u001b[0m           \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpywrap_tensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstatus\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 466\u001b[1;33m           pywrap_tensorflow.TF_GetCode(status))\n\u001b[0m\u001b[0;32m    467\u001b[0m   \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mResourceExhaustedError\u001b[0m: OOM when allocating tensor with shape[3639,3638]\n\t [[Node: mul_523 = Mul[T=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](sub_315, gradients_12/dense_62/MatMul_grad/MatMul_1)]]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-60-3682969d6ad7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbaseline_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdummy_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m50\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3000\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdummy_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mD:\\Anaconda3\\envs\\dlwin36\\lib\\site-packages\\keras\\models.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, **kwargs)\u001b[0m\n\u001b[0;32m    868\u001b[0m                               \u001b[0mclass_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    869\u001b[0m                               \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 870\u001b[1;33m                               initial_epoch=initial_epoch)\n\u001b[0m\u001b[0;32m    871\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    872\u001b[0m     def evaluate(self, x, y, batch_size=32, verbose=1,\n",
      "\u001b[1;32mD:\\Anaconda3\\envs\\dlwin36\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, **kwargs)\u001b[0m\n\u001b[0;32m   1505\u001b[0m                               \u001b[0mval_f\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mval_f\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_ins\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mval_ins\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1506\u001b[0m                               \u001b[0mcallback_metrics\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcallback_metrics\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1507\u001b[1;33m                               initial_epoch=initial_epoch)\n\u001b[0m\u001b[0;32m   1508\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m32\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\envs\\dlwin36\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[1;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch)\u001b[0m\n\u001b[0;32m   1154\u001b[0m                 \u001b[0mbatch_logs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'size'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1155\u001b[0m                 \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1156\u001b[1;33m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1157\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1158\u001b[0m                     \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\envs\\dlwin36\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2267\u001b[0m         updated = session.run(self.outputs + [self.updates_op],\n\u001b[0;32m   2268\u001b[0m                               \u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2269\u001b[1;33m                               **self.session_kwargs)\n\u001b[0m\u001b[0;32m   2270\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2271\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\envs\\dlwin36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    787\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    788\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 789\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    790\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    791\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\envs\\dlwin36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    995\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    996\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m--> 997\u001b[1;33m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[0;32m    998\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    999\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\envs\\dlwin36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1130\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1131\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[1;32m-> 1132\u001b[1;33m                            target_list, options, run_metadata)\n\u001b[0m\u001b[0;32m   1133\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1134\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[1;32mD:\\Anaconda3\\envs\\dlwin36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1150\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1151\u001b[0m           \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1152\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1153\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1154\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mResourceExhaustedError\u001b[0m: OOM when allocating tensor with shape[3639,3638]\n\t [[Node: mul_523 = Mul[T=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](sub_315, gradients_12/dense_62/MatMul_grad/MatMul_1)]]\n\nCaused by op 'mul_523', defined at:\n  File \"D:\\Anaconda3\\envs\\dlwin36\\lib\\runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"D:\\Anaconda3\\envs\\dlwin36\\lib\\runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"D:\\Anaconda3\\envs\\dlwin36\\lib\\site-packages\\ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"D:\\Anaconda3\\envs\\dlwin36\\lib\\site-packages\\traitlets\\config\\application.py\", line 658, in launch_instance\n    app.start()\n  File \"D:\\Anaconda3\\envs\\dlwin36\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 477, in start\n    ioloop.IOLoop.instance().start()\n  File \"D:\\Anaconda3\\envs\\dlwin36\\lib\\site-packages\\zmq\\eventloop\\ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"D:\\Anaconda3\\envs\\dlwin36\\lib\\site-packages\\tornado\\ioloop.py\", line 888, in start\n    handler_func(fd_obj, events)\n  File \"D:\\Anaconda3\\envs\\dlwin36\\lib\\site-packages\\tornado\\stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"D:\\Anaconda3\\envs\\dlwin36\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"D:\\Anaconda3\\envs\\dlwin36\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"D:\\Anaconda3\\envs\\dlwin36\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"D:\\Anaconda3\\envs\\dlwin36\\lib\\site-packages\\tornado\\stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"D:\\Anaconda3\\envs\\dlwin36\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"D:\\Anaconda3\\envs\\dlwin36\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 235, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"D:\\Anaconda3\\envs\\dlwin36\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"D:\\Anaconda3\\envs\\dlwin36\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"D:\\Anaconda3\\envs\\dlwin36\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 533, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"D:\\Anaconda3\\envs\\dlwin36\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2698, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"D:\\Anaconda3\\envs\\dlwin36\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2808, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"D:\\Anaconda3\\envs\\dlwin36\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2862, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-60-3682969d6ad7>\", line 2, in <module>\n    model.fit(X, dummy_train, batch_size=50, epochs=3000, validation_data=(X_test, dummy_test), callbacks=callbacks)\n  File \"D:\\Anaconda3\\envs\\dlwin36\\lib\\site-packages\\keras\\models.py\", line 870, in fit\n    initial_epoch=initial_epoch)\n  File \"D:\\Anaconda3\\envs\\dlwin36\\lib\\site-packages\\keras\\engine\\training.py\", line 1490, in fit\n    self._make_train_function()\n  File \"D:\\Anaconda3\\envs\\dlwin36\\lib\\site-packages\\keras\\engine\\training.py\", line 1014, in _make_train_function\n    self.total_loss)\n  File \"D:\\Anaconda3\\envs\\dlwin36\\lib\\site-packages\\keras\\optimizers.py\", line 422, in get_updates\n    m_t = (self.beta_1 * m) + (1. - self.beta_1) * g\n  File \"D:\\Anaconda3\\envs\\dlwin36\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py\", line 838, in binary_op_wrapper\n    return func(x, y, name=name)\n  File \"D:\\Anaconda3\\envs\\dlwin36\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py\", line 1061, in _mul_dispatch\n    return gen_math_ops._mul(x, y, name=name)\n  File \"D:\\Anaconda3\\envs\\dlwin36\\lib\\site-packages\\tensorflow\\python\\ops\\gen_math_ops.py\", line 1377, in _mul\n    result = _op_def_lib.apply_op(\"Mul\", x=x, y=y, name=name)\n  File \"D:\\Anaconda3\\envs\\dlwin36\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 767, in apply_op\n    op_def=op_def)\n  File \"D:\\Anaconda3\\envs\\dlwin36\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 2506, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"D:\\Anaconda3\\envs\\dlwin36\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1269, in __init__\n    self._traceback = _extract_stack()\n\nResourceExhaustedError (see above for traceback): OOM when allocating tensor with shape[3639,3638]\n\t [[Node: mul_523 = Mul[T=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](sub_315, gradients_12/dense_62/MatMul_grad/MatMul_1)]]\n"
     ]
    }
   ],
   "source": [
    "model = baseline_model()\n",
    "model.fit(X, dummy_train, batch_size=50, epochs=3000, validation_data=(X_test, dummy_test), callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = model.evaluate(X_test, dummy_y_test, batch_size=32)\n",
    "\n",
    "print('Accuracy score = {}'.format(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.873"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = model.predict(X_test)\n",
    "corrected_pred = uniques_test[numpy.round(pred).argmax(1)]\n",
    "accuracy_score(Y_test, corrected_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count of 1 = 1979\n",
      "Count of 0 = 262021\n"
     ]
    }
   ],
   "source": [
    "round_pred = numpy.round(pred)\n",
    "count_0 = 0\n",
    "count_1 = 0\n",
    "for i in numpy.nditer(round_pred):\n",
    "    if i == 1:\n",
    "        count_1 = count_1 + 1\n",
    "    else:\n",
    "        count_0 = count_0 + 1\n",
    "print(\"Count of 1 = {}\".format(count_1))\n",
    "print(\"Count of 0 = {}\".format(count_0))\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "# Support Vector Machines\n",
    "for i in range(1,150):\n",
    "    for j in range(1,15):\n",
    "        param_dist = {'n_estimators':i, 'nthread':22, 'max_depth':j}\n",
    "        xg = xgb.XGBClassifier(**param_dist)\n",
    "        xg.fit(X_train, Y_train, verbose=True)\n",
    "        X_test_ref = X_test.drop('user_id', axis=1)\n",
    "        Y_pred  = xg.predict(X_test_ref)\n",
    "        print \"accuracy {} with parameters {}\".format(accuracy_score(Y_test, Y_pred), param_dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_test_ref = X_test.drop('user_id', axis=1)\n",
    "xg_train = xgb.DMatrix(X_train.values, label=Y_train.values)\n",
    "xg_test = xgb.DMatrix(X_test_ref.values, label=Y_test)\n",
    "# setup parameters for xgboost\n",
    "param = {}\n",
    "# use softmax multi-class classification\n",
    "param['objective'] = 'multi:softmax'\n",
    "# scale weight of positive examples\n",
    "param['eta'] = 0.1\n",
    "param['max_depth'] = 6\n",
    "param['silent'] = 1\n",
    "param['nthread'] = 4\n",
    "param['num_class'] = len(dataframe['search_marka'].unique())\n",
    "\n",
    "watchlist = [(xg_train, 'train'), (xg_test, 'test')]\n",
    "num_round = 5\n",
    "bst = xgb.train(param, xg_train, num_round, watchlist)\n",
    "# get prediction\n",
    "pred = bst.predict(xg_test)\n",
    "error_rate = np.sum(pred != Y_test) / Y_test.shape[0]\n",
    "print('Test error using softmax = {}'.format(error_rate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training finished in 0 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.016666666666666666"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Support Vector Machines\n",
    "xg = rfc()\n",
    "d3 = datetime.now()\n",
    "xg.fit(X_train, Y_train)\n",
    "d4 = datetime.now()\n",
    "delta1 = d4 - d3\n",
    "print \"training finished in \" + str(delta1.seconds) + \" seconds\"\n",
    "X_test_rfc_ref = X_test.drop('user_id', axis=1)\n",
    "Y_rfc_pred  = xg.predict(X_test_rfc_ref)\n",
    "\n",
    "submission_rfc = pd.DataFrame({\n",
    "         \"user_id\": X_test[\"user_id\"],\n",
    "         \"search_marka\": Y_rfc_pred\n",
    "     })\n",
    "submission_rfc.to_csv('resource/results_RFC.csv', index=False)\n",
    "accuracy_score(Y_test, Y_rfc_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "transform() takes exactly 2 arguments (3 given)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-45c6ec862808>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0menc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mohe\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0menc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataframe\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'search_marka'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdataframe\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'search_model'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: transform() takes exactly 2 arguments (3 given)"
     ]
    }
   ],
   "source": [
    "enc = ohe()\n",
    "enc.transform(dataframe['search_marka'], dataframe['search_model'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.97997496871088863"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_pred  = xg.predict(X_train)\n",
    "\n",
    "submission = pd.DataFrame({\n",
    "         \"user_id\": X_train[\"user_id\"],\n",
    "         \"search_marka\": Y_pred\n",
    "     })\n",
    "submission.to_csv('resource/results_XGB.csv', index=False)\n",
    "xg.score(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "plot_histograms() takes exactly 4 arguments (1 given)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-e08f45a664f1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpreprocessing\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscale\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataframe\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'search_year_from'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mplot_histograms\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: plot_histograms() takes exactly 4 arguments (1 given)"
     ]
    }
   ],
   "source": [
    "result = preprocessing.scale(dataframe['search_year_from'])\n",
    "plot_histograms(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "132"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataframe['search_marka'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "last_login             int64\n",
       "search_body            int32\n",
       "search_city            int32\n",
       "search_country         int32\n",
       "search_model           int32\n",
       "search_rigion          int32\n",
       "search_transmission    int32\n",
       "search_wheel           int32\n",
       "search_volume          int32\n",
       "search_milleage        int32\n",
       "search_price           int32\n",
       "search_year            int32\n",
       "a                      int32\n",
       "b                      int32\n",
       "c                      int32\n",
       "d                      int32\n",
       "f                      int32\n",
       "e                      int32\n",
       "dtype: object"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     last_login  search_body  search_city  search_country  search_marka  \\\n",
      "0             2            5           17               1            96   \n",
      "1             2            5           14               1            26   \n",
      "2             0            1            2               2            70   \n",
      "3             1            8           17               2           100   \n",
      "4             2            8           10               2            71   \n",
      "5             3            7           23               2            41   \n",
      "6             3            2           53               2            78   \n",
      "7             1            3           14               2            45   \n",
      "8             0            1           32               2           126   \n",
      "9             0            6            1               1           103   \n",
      "10            3           10            1               1            31   \n",
      "11            3            8            1               1           131   \n",
      "12            0            5           13               1           129   \n",
      "13            2           11            1               1            14   \n",
      "14            0            6            4               2            96   \n",
      "15            1            9            6               1            61   \n",
      "16            3            8            5               1            37   \n",
      "17            2            2            3               1            56   \n",
      "18            1           10            1               2            83   \n",
      "19            1            4            2               1            57   \n",
      "20            1            7           35               2           118   \n",
      "21            1            2            4               1           121   \n",
      "22            2            3           14               2           107   \n",
      "23            3            9            6               1            14   \n",
      "24            3            9            9               1           101   \n",
      "25            2            7           24               1           112   \n",
      "26            3            8           51               2            87   \n",
      "27            1            2           35               2            84   \n",
      "28            3            7           13               1            29   \n",
      "29            2           10           16               2           100   \n",
      "..          ...          ...          ...             ...           ...   \n",
      "269           3            6           19               1           101   \n",
      "270           2            8           18               1            36   \n",
      "271           2            1           24               1            28   \n",
      "272           1           11            1               1            80   \n",
      "273           3            1           11               1            69   \n",
      "274           2           11            5               2            96   \n",
      "275           1            1           13               1            45   \n",
      "276           0            6           11               1           115   \n",
      "277           1            5            1               2             5   \n",
      "278           1            6           44               2            89   \n",
      "279           2            7            2               2            59   \n",
      "280           3            7            7               2            31   \n",
      "281           2            1           18               2           123   \n",
      "282           1           10           15               1            14   \n",
      "283           0           10            1               2           107   \n",
      "284           1           11            2               2            61   \n",
      "285           2            7           11               1            52   \n",
      "286           0           11           19               2            28   \n",
      "287           0           11           27               2            91   \n",
      "288           2            9            4               2            21   \n",
      "289           2            1            4               2            61   \n",
      "290           0            4            4               1             6   \n",
      "291           2            9            8               2           120   \n",
      "292           1            3            8               1            16   \n",
      "293           0            6           85               2            84   \n",
      "294           3            2           20               1            11   \n",
      "295           0            8           22               1            35   \n",
      "296           0            7            1               1            42   \n",
      "297           1            8            1               1           129   \n",
      "298           1           10           26               1           105   \n",
      "\n",
      "     search_model  search_rigion  search_transmission  search_wheel  user_id  \\\n",
      "0               3              2                    1             2        1   \n",
      "1               3              3                    1             1        2   \n",
      "2              82             50                    1             3        3   \n",
      "3               1             39                    1             2        4   \n",
      "4               7             48                    1             1        5   \n",
      "5               7             51                    2             3        6   \n",
      "6              27             28                    1             2        7   \n",
      "7               2             80                    1             3        8   \n",
      "8               7             15                    1             1        9   \n",
      "9               8              1                    1             1       10   \n",
      "10              1              1                    2             3       11   \n",
      "11              2              1                    2             1       12   \n",
      "12              3              6                    1             2       13   \n",
      "13              1              1                    2             3       14   \n",
      "14             10             10                    1             2       15   \n",
      "15              2              6                    1             1       16   \n",
      "16              2              7                    1             2       17   \n",
      "17              1              7                    1             2       18   \n",
      "18             16             14                    1             1       19   \n",
      "19              1              4                    1             2       20   \n",
      "20              2             30                    1             1       21   \n",
      "21              1              6                    2             2       22   \n",
      "22             90             77                    1             3       23   \n",
      "23              2              7                    2             3       24   \n",
      "24              5              7                    1             3       25   \n",
      "25              5              2                    1             2       26   \n",
      "26              9             65                    1             1       27   \n",
      "27              3             11                    1             3       28   \n",
      "28             16              5                    2             1       29   \n",
      "29              1             45                    2             3       30   \n",
      "..            ...            ...                  ...           ...      ...   \n",
      "269             6              5                    2             2      270   \n",
      "270             3              3                    2             2      271   \n",
      "271             2              2                    1             1      272   \n",
      "272             1              3                    2             1      273   \n",
      "273            15              7                    1             2      274   \n",
      "274             4             20                    2             3      275   \n",
      "275            74              5                    2             3      276   \n",
      "276             1              7                    1             2      277   \n",
      "277             2             41                    2             3      278   \n",
      "278             2              6                    2             1      279   \n",
      "279             1             45                    2             3      280   \n",
      "280             1             76                    1             1      281   \n",
      "281             2             35                    2             2      282   \n",
      "282             1              7                    2             3      283   \n",
      "283             3              2                    1             3      284   \n",
      "284             2             26                    2             1      285   \n",
      "285             4              4                    1             1      286   \n",
      "286             1             43                    2             2      287   \n",
      "287             4             61                    1             2      288   \n",
      "288            17             19                    2             2      289   \n",
      "289             2             17                    2             1      290   \n",
      "290            10              4                    1             1      291   \n",
      "291             1             24                    2             1      292   \n",
      "292             2              6                    2             1      293   \n",
      "293            11             31                    2             2      294   \n",
      "294             3              6                    1             2      295   \n",
      "295             1              4                    1             2      296   \n",
      "296             1              1                    1             2      297   \n",
      "297             2              2                    1             1      298   \n",
      "298             3              4                    1             3      299   \n",
      "\n",
      "     search_volume  search_milleage  search_price  search_year  \n",
      "0                0                3             3            3  \n",
      "1                0                3             2            2  \n",
      "2                0                1             0            1  \n",
      "3                3                1             2            2  \n",
      "4                2                0             3            1  \n",
      "5                1                2             1            1  \n",
      "6                1                2             1            3  \n",
      "7                3                2             2            3  \n",
      "8                0                3             2            1  \n",
      "9                1                1             3            0  \n",
      "10               3                1             3            2  \n",
      "11               1                0             1            3  \n",
      "12               1                2             2            2  \n",
      "13               3                0             0            3  \n",
      "14               0                1             3            0  \n",
      "15               2                3             3            3  \n",
      "16               1                2             3            2  \n",
      "17               0                1             2            2  \n",
      "18               3                3             0            3  \n",
      "19               2                3             2            2  \n",
      "20               3                3             3            2  \n",
      "21               0                3             3            1  \n",
      "22               1                0             0            1  \n",
      "23               3                2             3            0  \n",
      "24               3                2             3            0  \n",
      "25               0                1             2            2  \n",
      "26               1                1             2            0  \n",
      "27               1                2             3            0  \n",
      "28               3                0             3            3  \n",
      "29               1                3             2            0  \n",
      "..             ...              ...           ...          ...  \n",
      "269              1                2             3            3  \n",
      "270              0                1             2            0  \n",
      "271              1                2             3            2  \n",
      "272              2                1             1            1  \n",
      "273              1                3             1            2  \n",
      "274              2                1             2            1  \n",
      "275              0                0             3            1  \n",
      "276              1                2             1            3  \n",
      "277              0                2             2            0  \n",
      "278              2                0             2            3  \n",
      "279              1                1             3            0  \n",
      "280              0                0             2            1  \n",
      "281              0                3             1            3  \n",
      "282              1                1             0            1  \n",
      "283              0                3             3            1  \n",
      "284              1                0             3            1  \n",
      "285              2                0             0            2  \n",
      "286              0                3             0            1  \n",
      "287              1                0             1            0  \n",
      "288              2                1             2            0  \n",
      "289              0                3             2            1  \n",
      "290              1                2             1            3  \n",
      "291              0                3             2            0  \n",
      "292              2                2             2            2  \n",
      "293              3                0             3            1  \n",
      "294              3                1             1            2  \n",
      "295              0                3             3            3  \n",
      "296              3                1             0            3  \n",
      "297              3                0             0            3  \n",
      "298              3                0             2            2  \n",
      "\n",
      "[299 rows x 14 columns]\n"
     ]
    }
   ],
   "source": [
    "printer = PrettyPrinter()\n",
    "printer.pprint(dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plot_correlation_map( dataframe )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>last_login</th>\n",
       "      <th>search_milleage_from</th>\n",
       "      <th>search_milleage_to</th>\n",
       "      <th>search_price_from</th>\n",
       "      <th>search_price_to</th>\n",
       "      <th>search_volume_from</th>\n",
       "      <th>search_volume_to</th>\n",
       "      <th>search_year_from</th>\n",
       "      <th>search_year_to</th>\n",
       "      <th>user_id</th>\n",
       "      <th>search_volume</th>\n",
       "      <th>search_milleage</th>\n",
       "      <th>search_price</th>\n",
       "      <th>search_year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>2.990000e+02</td>\n",
       "      <td>299.000000</td>\n",
       "      <td>299.000000</td>\n",
       "      <td>299.000000</td>\n",
       "      <td>299.000000</td>\n",
       "      <td>299.000000</td>\n",
       "      <td>299.000000</td>\n",
       "      <td>299.000000</td>\n",
       "      <td>299.000000</td>\n",
       "      <td>299.000000</td>\n",
       "      <td>299.000000</td>\n",
       "      <td>299.000000</td>\n",
       "      <td>299.000000</td>\n",
       "      <td>299.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1.347289e+09</td>\n",
       "      <td>488465.541806</td>\n",
       "      <td>742932.829431</td>\n",
       "      <td>200902.591973</td>\n",
       "      <td>303719.210702</td>\n",
       "      <td>3930.284281</td>\n",
       "      <td>6108.317726</td>\n",
       "      <td>1987.324415</td>\n",
       "      <td>2001.558528</td>\n",
       "      <td>150.000000</td>\n",
       "      <td>5019.301003</td>\n",
       "      <td>615699.185619</td>\n",
       "      <td>252310.901338</td>\n",
       "      <td>1994.441472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>8.776082e+07</td>\n",
       "      <td>291487.259417</td>\n",
       "      <td>223817.709339</td>\n",
       "      <td>116748.924709</td>\n",
       "      <td>88933.634518</td>\n",
       "      <td>2352.715403</td>\n",
       "      <td>1773.052419</td>\n",
       "      <td>16.234805</td>\n",
       "      <td>12.148373</td>\n",
       "      <td>86.458082</td>\n",
       "      <td>1865.765792</td>\n",
       "      <td>234020.185247</td>\n",
       "      <td>93562.283804</td>\n",
       "      <td>12.813351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.201130e+09</td>\n",
       "      <td>2651.000000</td>\n",
       "      <td>83537.000000</td>\n",
       "      <td>949.000000</td>\n",
       "      <td>2488.000000</td>\n",
       "      <td>42.000000</td>\n",
       "      <td>465.000000</td>\n",
       "      <td>1960.000000</td>\n",
       "      <td>1963.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>451.500000</td>\n",
       "      <td>53586.000000</td>\n",
       "      <td>2210.000000</td>\n",
       "      <td>1961.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.274145e+09</td>\n",
       "      <td>244253.000000</td>\n",
       "      <td>609723.000000</td>\n",
       "      <td>103131.500000</td>\n",
       "      <td>247352.000000</td>\n",
       "      <td>1823.000000</td>\n",
       "      <td>5413.000000</td>\n",
       "      <td>1973.500000</td>\n",
       "      <td>1995.000000</td>\n",
       "      <td>75.500000</td>\n",
       "      <td>3759.500000</td>\n",
       "      <td>457799.000000</td>\n",
       "      <td>182155.000000</td>\n",
       "      <td>1985.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.351174e+09</td>\n",
       "      <td>473300.000000</td>\n",
       "      <td>797842.000000</td>\n",
       "      <td>185065.000000</td>\n",
       "      <td>332748.000000</td>\n",
       "      <td>3965.000000</td>\n",
       "      <td>6697.000000</td>\n",
       "      <td>1986.000000</td>\n",
       "      <td>2005.000000</td>\n",
       "      <td>150.000000</td>\n",
       "      <td>5172.000000</td>\n",
       "      <td>616495.000000</td>\n",
       "      <td>257996.000000</td>\n",
       "      <td>1994.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.414614e+09</td>\n",
       "      <td>728773.500000</td>\n",
       "      <td>931896.000000</td>\n",
       "      <td>305288.000000</td>\n",
       "      <td>373312.000000</td>\n",
       "      <td>5922.500000</td>\n",
       "      <td>7486.000000</td>\n",
       "      <td>2001.500000</td>\n",
       "      <td>2011.000000</td>\n",
       "      <td>224.500000</td>\n",
       "      <td>6634.250000</td>\n",
       "      <td>817742.500000</td>\n",
       "      <td>331533.250000</td>\n",
       "      <td>2005.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.500867e+09</td>\n",
       "      <td>994869.000000</td>\n",
       "      <td>999148.000000</td>\n",
       "      <td>398572.000000</td>\n",
       "      <td>399883.000000</td>\n",
       "      <td>7990.000000</td>\n",
       "      <td>7998.000000</td>\n",
       "      <td>2016.000000</td>\n",
       "      <td>2016.000000</td>\n",
       "      <td>299.000000</td>\n",
       "      <td>7994.000000</td>\n",
       "      <td>995650.000000</td>\n",
       "      <td>399227.500000</td>\n",
       "      <td>2016.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         last_login  search_milleage_from  search_milleage_to  \\\n",
       "count  2.990000e+02            299.000000          299.000000   \n",
       "mean   1.347289e+09         488465.541806       742932.829431   \n",
       "std    8.776082e+07         291487.259417       223817.709339   \n",
       "min    1.201130e+09           2651.000000        83537.000000   \n",
       "25%    1.274145e+09         244253.000000       609723.000000   \n",
       "50%    1.351174e+09         473300.000000       797842.000000   \n",
       "75%    1.414614e+09         728773.500000       931896.000000   \n",
       "max    1.500867e+09         994869.000000       999148.000000   \n",
       "\n",
       "       search_price_from  search_price_to  search_volume_from  \\\n",
       "count         299.000000       299.000000          299.000000   \n",
       "mean       200902.591973    303719.210702         3930.284281   \n",
       "std        116748.924709     88933.634518         2352.715403   \n",
       "min           949.000000      2488.000000           42.000000   \n",
       "25%        103131.500000    247352.000000         1823.000000   \n",
       "50%        185065.000000    332748.000000         3965.000000   \n",
       "75%        305288.000000    373312.000000         5922.500000   \n",
       "max        398572.000000    399883.000000         7990.000000   \n",
       "\n",
       "       search_volume_to  search_year_from  search_year_to     user_id  \\\n",
       "count        299.000000        299.000000      299.000000  299.000000   \n",
       "mean        6108.317726       1987.324415     2001.558528  150.000000   \n",
       "std         1773.052419         16.234805       12.148373   86.458082   \n",
       "min          465.000000       1960.000000     1963.000000    1.000000   \n",
       "25%         5413.000000       1973.500000     1995.000000   75.500000   \n",
       "50%         6697.000000       1986.000000     2005.000000  150.000000   \n",
       "75%         7486.000000       2001.500000     2011.000000  224.500000   \n",
       "max         7998.000000       2016.000000     2016.000000  299.000000   \n",
       "\n",
       "       search_volume  search_milleage   search_price  search_year  \n",
       "count     299.000000       299.000000     299.000000   299.000000  \n",
       "mean     5019.301003    615699.185619  252310.901338  1994.441472  \n",
       "std      1865.765792    234020.185247   93562.283804    12.813351  \n",
       "min       451.500000     53586.000000    2210.000000  1961.500000  \n",
       "25%      3759.500000    457799.000000  182155.000000  1985.500000  \n",
       "50%      5172.000000    616495.000000  257996.000000  1994.500000  \n",
       "75%      6634.250000    817742.500000  331533.250000  2005.000000  \n",
       "max      7994.000000    995650.000000  399227.500000  2016.000000  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframe.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[<matplotlib.axes._subplots.AxesSubplot object at 0x000000000E528FD0>,\n",
       "        <matplotlib.axes._subplots.AxesSubplot object at 0x000000000E746A20>,\n",
       "        <matplotlib.axes._subplots.AxesSubplot object at 0x000000000E846588>,\n",
       "        <matplotlib.axes._subplots.AxesSubplot object at 0x000000000EAE50F0>],\n",
       "       [<matplotlib.axes._subplots.AxesSubplot object at 0x00000000104C7748>,\n",
       "        <matplotlib.axes._subplots.AxesSubplot object at 0x0000000010591240>,\n",
       "        <matplotlib.axes._subplots.AxesSubplot object at 0x0000000010641390>,\n",
       "        <matplotlib.axes._subplots.AxesSubplot object at 0x000000001073DDA0>],\n",
       "       [<matplotlib.axes._subplots.AxesSubplot object at 0x00000000108304A8>,\n",
       "        <matplotlib.axes._subplots.AxesSubplot object at 0x00000000108F3F60>,\n",
       "        <matplotlib.axes._subplots.AxesSubplot object at 0x00000000109A84E0>,\n",
       "        <matplotlib.axes._subplots.AxesSubplot object at 0x0000000010AB3080>],\n",
       "       [<matplotlib.axes._subplots.AxesSubplot object at 0x0000000010B568D0>,\n",
       "        <matplotlib.axes._subplots.AxesSubplot object at 0x0000000010C5F518>,\n",
       "        <matplotlib.axes._subplots.AxesSubplot object at 0x0000000010BB4128>,\n",
       "        <matplotlib.axes._subplots.AxesSubplot object at 0x0000000010E14400>],\n",
       "       [<matplotlib.axes._subplots.AxesSubplot object at 0x0000000010ECFEB8>,\n",
       "        <matplotlib.axes._subplots.AxesSubplot object at 0x0000000011001F60>,\n",
       "        <matplotlib.axes._subplots.AxesSubplot object at 0x00000000110CB9B0>,\n",
       "        <matplotlib.axes._subplots.AxesSubplot object at 0x000000000D2187B8>]], dtype=object)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfQAAAF1CAYAAAAeOhj3AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nZ4wxxgzA/wNEbRZWlz8x5gAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0xe575b38>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pandas.DataFrame.hist(dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "type numpy.ndarray doesn't define __round__ method",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-57-75314aa7cbfc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mY_pred_rnd\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mround\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mY_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: type numpy.ndarray doesn't define __round__ method"
     ]
    }
   ],
   "source": [
    "Y_pred_rnd = round(Y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>last_login</th>\n",
       "      <th>search_body</th>\n",
       "      <th>search_country</th>\n",
       "      <th>search_marka</th>\n",
       "      <th>search_transmission</th>\n",
       "      <th>search_wheel</th>\n",
       "      <th>user_id</th>\n",
       "      <th>search_volume</th>\n",
       "      <th>search_milleage</th>\n",
       "      <th>search_price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>last_login</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.115474</td>\n",
       "      <td>0.022384</td>\n",
       "      <td>-0.071284</td>\n",
       "      <td>-0.046494</td>\n",
       "      <td>-0.020060</td>\n",
       "      <td>-0.017998</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>search_body</th>\n",
       "      <td>-0.115474</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.001915</td>\n",
       "      <td>-0.055928</td>\n",
       "      <td>-0.049403</td>\n",
       "      <td>-0.033310</td>\n",
       "      <td>-0.036047</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>search_country</th>\n",
       "      <td>0.022384</td>\n",
       "      <td>-0.001915</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.072432</td>\n",
       "      <td>-0.037934</td>\n",
       "      <td>-0.036925</td>\n",
       "      <td>-0.098124</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>search_marka</th>\n",
       "      <td>-0.071284</td>\n",
       "      <td>-0.055928</td>\n",
       "      <td>0.072432</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.045894</td>\n",
       "      <td>-0.053708</td>\n",
       "      <td>0.038386</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>search_transmission</th>\n",
       "      <td>-0.046494</td>\n",
       "      <td>-0.049403</td>\n",
       "      <td>-0.037934</td>\n",
       "      <td>0.045894</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.076887</td>\n",
       "      <td>0.042787</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>search_wheel</th>\n",
       "      <td>-0.020060</td>\n",
       "      <td>-0.033310</td>\n",
       "      <td>-0.036925</td>\n",
       "      <td>-0.053708</td>\n",
       "      <td>0.076887</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.021646</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>user_id</th>\n",
       "      <td>-0.017998</td>\n",
       "      <td>-0.036047</td>\n",
       "      <td>-0.098124</td>\n",
       "      <td>0.038386</td>\n",
       "      <td>0.042787</td>\n",
       "      <td>-0.021646</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>search_volume</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>search_milleage</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>search_price</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     last_login  search_body  search_country  search_marka  \\\n",
       "last_login             1.000000    -0.115474        0.022384     -0.071284   \n",
       "search_body           -0.115474     1.000000       -0.001915     -0.055928   \n",
       "search_country         0.022384    -0.001915        1.000000      0.072432   \n",
       "search_marka          -0.071284    -0.055928        0.072432      1.000000   \n",
       "search_transmission   -0.046494    -0.049403       -0.037934      0.045894   \n",
       "search_wheel          -0.020060    -0.033310       -0.036925     -0.053708   \n",
       "user_id               -0.017998    -0.036047       -0.098124      0.038386   \n",
       "search_volume               NaN          NaN             NaN           NaN   \n",
       "search_milleage             NaN          NaN             NaN           NaN   \n",
       "search_price                NaN          NaN             NaN           NaN   \n",
       "\n",
       "                     search_transmission  search_wheel   user_id  \\\n",
       "last_login                     -0.046494     -0.020060 -0.017998   \n",
       "search_body                    -0.049403     -0.033310 -0.036047   \n",
       "search_country                 -0.037934     -0.036925 -0.098124   \n",
       "search_marka                    0.045894     -0.053708  0.038386   \n",
       "search_transmission             1.000000      0.076887  0.042787   \n",
       "search_wheel                    0.076887      1.000000 -0.021646   \n",
       "user_id                         0.042787     -0.021646  1.000000   \n",
       "search_volume                        NaN           NaN       NaN   \n",
       "search_milleage                      NaN           NaN       NaN   \n",
       "search_price                         NaN           NaN       NaN   \n",
       "\n",
       "                     search_volume  search_milleage  search_price  \n",
       "last_login                     NaN              NaN           NaN  \n",
       "search_body                    NaN              NaN           NaN  \n",
       "search_country                 NaN              NaN           NaN  \n",
       "search_marka                   NaN              NaN           NaN  \n",
       "search_transmission            NaN              NaN           NaN  \n",
       "search_wheel                   NaN              NaN           NaN  \n",
       "user_id                        NaN              NaN           NaN  \n",
       "search_volume                  NaN              NaN           NaN  \n",
       "search_milleage                NaN              NaN           NaN  \n",
       "search_price                   NaN              NaN           NaN  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframe.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(299, 20)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframe.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
